{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Application Claim Docs \u00b6 Introduction \u00b6 The motiviation for this project was to research application claim pattern. This pattern allows applciations to have a ligth weight claim that is used create a much more detailed resource. The deployment environment that is targeted is Kubernetes and the resources that are targeted by these claims are infrastructure resources like AWS RDS, S3 and other cloud provider services. The claim pattern shields the applications from knowing anything about the cloud provider concerns that are typically managed by a differents team that deploys the applciations and manages cloud infrastructure. There are examples of this pattern for example crossplane composition aims to support this claim pattern. Another example of this pattern is the db-controller and its DatabaseClaim . The former crossplane claim pattern is very general purpose and more passive while the db-controller claim is very specialized for database application and db-controller takes a more active role in managing the resource and provides database proxy and secret key rotation services. In this project we will research some other patterns, the goal is to find a common pattern(s) for application deployment. Operators versus Declarative Configuration \u00b6 In the following analysis we will consider declarative configuration of infrastructure from claims. This type of deployment might not fit some (or many) use cases where resources are shared and there is a need for reclaim policies enforced by an Operator, or there is specialized processing of the infrastructure resources required e.g. health monitoring or backup. It is intersting that crossplane composition reclaim lifecycle was backed out recently . We will examine this decision once we have found all our use cases that will require the claim pattern. In this project we will define CRDs for the claims that we will use in the declarative configuration. This will allow us to load them on the cluster and if necessary process them using an Operator. We will seed the project with the kubebuilder db-controller project scaffolding so we can use kubebuilder to build CRDs quickly but we will not implement any runtime logic in this proejct. The db-controller was setup for a single resource and we enable the project for multigroup layout: kubebuilder edit --multigroup true There is work to refactor the definition from /api to /apis directory but since this is PoC, I will not make these changes here. We want the claim data model to follow the following schema: %%{init: {'theme': 'base', 'themeVariables': { 'primaryColor': '#fffcbb', 'lineColor': '#ff0000', 'primaryBorderColor': '#ff0000'}}}%% graph LR A[[claims.atlas.infoblox.com]] --> B((ObjectStoreClaim)) A --> C((DataBaseClaim)) We could have finer grained grouping: %%{init: {'theme': 'base', 'themeVariables': { 'primaryColor': '#fffcbb', 'lineColor': '#ff0000', 'primaryBorderColor': '#ff0000'}}}%% graph LR A[[claims.atlas.infoblox.com]] --> B[[persistence.claims.atlas.infoblox.com]] B --> C((ObjectStoreClaim)) B --> D((DataBaseClaim)) For now we go with the coarse model, as most of the work is defining the leaf nodes. We can refactor to the finer grain model once we have a better idea of the different claims needed for the applications. We start by creating a claim for ObjectStore which we would map to AWS S3 resource for example: kubebuilder create api --controller false --kind ObjectStoreClaim --version v1 --group objectstore.claims.atlas.infoblox.com make update_crds Additional Patterns \u00b6 Generally infrastructure resources are injected into applications using configmap or secrets. In addition to the pattern highlighted above we will look at two other patterns that build on the established configmap or secret pattern. One problem with these patterns is that the namespace and associated configmap or secret must be present for the application deployment to succeed as the dependency is not present in the helm chart that is resolved. The two approaches we will make is to: - Use a workflow engine to process application defintion e.g. helm chart and use the application claim to create the create the resource - Custom program that understands applications and the environment consumes application helm charts and emits resolved mainfest that coverts claims to resources In general this the state flow in handling all the applications, in both approaches: %%{init: {'theme': 'base', 'themeVariables': { 'primaryColor': '#fffcbb', 'lineColor': '#ff0000', 'primaryBorderColor': '#ff0000'}}}%% stateDiagram-v2 [*] --> ProcessDC ProcessDC --> ProcessNextApp ProcessNextApp --> CheckNextApp state CheckNextApp < > CheckNextApp --> ProcessApp: if NextApp != \"\" CheckNextApp --> [*] : if NextApp == \"\" ProcessApp --> FetchChart FetchChart --> ExpandChart ExpandChart --> Customize Customize --> GitCheckIn GitCheckIn --> ProcessNextApp Workflow Approach \u00b6 In the workflow approach we would pick a solution that has a decralative workflow engine, two open source solutions are good examples: - Kubevela - Argo Workflow The two workflows solutions have different trades offs, kubevela is more specialized to CI/CD delivery has native integration with helm and other tooling using its plugin architecture, while argo workflow is more generalized and flexible in building workflows. The kubevela schema defintion could be a benefit if its fits your deployment model or problem as you have to define and model your configuration. Custom Approach \u00b6 In the custom approach we will build an application that understands the data model for our deployment configuration repo (dc-repo), see this example of dc-repo . This custom solution can still use open source tooling like kpt or QUE for declartive defintion and processing of the helm chart and creating resource configuration. We could leverage QUE integration in kubevela or develope our own custom integration . The kpt book and poc are good resources. We will focus on kpt helm support as this will be the source of our application defintion. I will focus on kpt and how it can be integrated, as I feel that the QUE language and integration in kubevela is better understood at this time. The kpt tool needs to have a resolved manifest for it to operate. We can step through the test CMDB repo, note with CMDB when built for local testing that postgres local server is run with the deployment, when run in the cloud it is run with a claim. To enable this we set postgresql.enabled to false: mkdir deploy cd tmp git clone git@github.com:seizadi/cmdb.git mkdir build cd build helm template --set postgresql.enabled = false -n cmdb ../cmdb/repo/cmdb/. --output-dir . At this point this is what our directory structure would look like: \u276f tree cmdb cmdb \u2514\u2500\u2500 templates \u251c\u2500\u2500 configmap.yaml \u251c\u2500\u2500 deployment.yaml \u251c\u2500\u2500 ingress.yaml \u251c\u2500\u2500 migrations.yaml \u251c\u2500\u2500 ns.yaml \u251c\u2500\u2500 postgres-claim.yaml \u251c\u2500\u2500 service.yaml \u2514\u2500\u2500 serviceaccount.yaml We are really intersted in the claim in postgres-claim.yaml: --- # Source: cmdb/templates/postgres-claim.yaml apiVersion : database.example.org/v1alpha1 kind : PostgresInstance metadata : name : RELEASE-NAME-cmdb namespace : cmdb spec : parameters : storageGB : 20 writeConnectionSecretToRef : name : RELEASE-NAME-cmdb-postgres-con The goal now is to kpt to process this claim and convert it to a infrastructure claim. We will use the kpt function pattern to search and replace the claim above with an infrastructure defintion that looks like this: apiVersion : database.aws.crossplane.io/v1beta1 kind : RDSInstance metadata : name : rdspostgresql spec : forProvider : region : us-west-1 vpcSecurityGroupIDRefs : - name : seizadi-bloxinabox-rds-sg dbSubnetGroupNameRef : name : seizadi-bloxinabox-rds-subnetgroup dbInstanceClass : <shape goes here> masterUsername : masteruser allocatedStorage : <minStorageGB goes here> engine : postgres engineVersion : \"12.8\" skipFinalSnapshotBeforeDeletion : true publiclyAccessible : false # enableIAMDatabaseAuthentication: true writeConnectionSecretToRef : namespace : <claim namespace goes here> name : <writeConnectionSecretToRef goes here> providerConfigRef : name : default The kpt function catalog is a good place to look for features to build for our required mutations. We can do some processing of the helm chart as needed like mutating the namespace for the application or setting it release name: kpt fn eval --image gcr.io/kpt-fn/search-replace:v0.2.0 -- by-path = 'metadata.namespace' put-value = 'cmdb' kpt fn eval --image gcr.io/kpt-fn/search-replace:v0.2.0 -- by-path = 'metadata.name' put-value = 'cmdb-dev-seizadi' You should see changes like this: diff --git a/deploy/build/cmdb/templates/configmap.yaml b/deploy/build/cmdb/templates/configmap.yaml index 3ad3e33..813bf80 100644 --- a/deploy/build/cmdb/templates/configmap.yaml +++ b/deploy/build/cmdb/templates/configmap.yaml @@ -3,7 +3,7 @@ apiVersion: v1 kind: ConfigMap metadata: name: RELEASE-NAME-cmdb - namespace: default + namespace: cmdb We try to do the claim mutation by using apply-replacements function which uses the kustomize synthax for replacements. apiVersion : kpt.dev/v1 kind : Kptfile metadata : name : claim pipeline : mutators : - image : gcr.io/kpt-fn/apply-replacements:unstable configPath : replacements.yaml We will define a kpt file to detail the desired changes: apiVersion : kpt.dev/v1 kind : Kptfile metadata : name : claim pipeline : mutators : - image : gcr.io/kpt-fn/apply-replacements:unstable configPath : replacements.yaml Now we can process the claim: kpt fn render claim Now we can check what changed in the infrastructure defintion was updated: diff --git a/deploy/claim/infrastructure.yaml b/deploy/claim/infrastructure.yaml index e705744..ad2dce9 100644 --- a/deploy/claim/infrastructure.yaml +++ b/deploy/claim/infrastructure.yaml @@ -10,16 +10,16 @@ spec: - name: seizadi-bloxinabox-rds-sg dbSubnetGroupNameRef: name: seizadi-bloxinabox-rds-subnetgroup - dbInstanceClass: <shape goes here> + dbInstanceClass: db.t2.small masterUsername: masteruser - allocatedStorage: <minStorageGB goes here> + allocatedStorage: 20 engine: postgres engineVersion: \"12.8\" skipFinalSnapshotBeforeDeletion: true publiclyAccessible: false # enableIAMDatabaseAuthentication: true writeConnectionSecretToRef: - namespace: <claim namespace goes here> - name: <writeConnectionSecretToRef goes here> + namespace: cmdb + name: RELEASE-NAME-cmdb-postgres-con providerConfigRef: name: default This shows simple mutations but there might be cases where you need to implement conditional logic or more complex mutations rather than replace/substitutions. For example see the more complex operations available to crossplane compositions to transform claims. In these cases in the case of kpt we have starlark scripting . starlark is is a dialect of Python. It is commonly used as a configuration language. It is an untyped dynamic language with high-level data types, first-class functions with lexical scope, and garbage collection. You can find the spec for the Starlark language here . You can also find its API reference here . We will do a example of claim transformation using this method as well. POC USING STARLARK SCRIPT GOES HERE! Application Use Cases \u00b6 Lets look at some common use cases below Container Args \u00b6 In this use case you have something like this: containers : - name : <some-container> ..... args : - --s3-region=<some region> - --s3-bucket=<some bucket> - --s3-folder=<some folder> ..... There is also the associated secret and you can have this injected via Secrets, use node IAM permission, kube2iam or IRSA. This declaration does not tell us if the S3 resource is shared by other applciations. If it is shared does not application need read only or read/write access to the resource. The application claim should also include these intents. The clients will use AWS SDK to access the object store, we have to test other cloud providers but they are suppose to be able to support the API calls with their own endpoints: import ( \"github.com/aws/aws-sdk-go/aws\" \"github.com/aws/aws-sdk-go/aws/session\" \"github.com/aws/aws-sdk-go/service/s3\" \"github.com/aws/aws-sdk-go/service/s3/s3manager\" ) ... awsConfig := & aws . Config { Region : aws . String ( s3 - region ), HTTPClient : & http . Client { Timeout : defaultHTTPTimeout }, Endpoint : & config . Endpoint , } awsSession , _ := session . NewSession ( awsConfig ) svc = s3 . New ( awsSession ) downloader = s3manager . NewDownloader ( awsSession ) ... The ConfigMap pattern is another one found in applications, you can refrence the open source solution Teleport in how it pulls persistence information for its configuration: --- # Source: teleport/templates/config.yaml apiVersion : v1 kind : ConfigMap metadata : name : teleport data : teleport.yaml : | teleport: log: output: stderr severity: DEBUG data_dir: /var/lib/teleport auth_servers: - teleport.ib-system.svc.cluster.local:3025 - auth-poseidon-preprod.csp.infoblox.com:3025 storage: audit_events_uri: - \"dynamodb://<some-dynamodb>\" audit_sessions_uri: \"s3://<some-bucket>\" Another example similarly for configuration of FOSSA using ConfigMap: --- # Source: fossa-core/templates/config/config-configmap.yaml apiVersion : v1 kind : ConfigMap metadata : name : fossa-config namespace : fossa data : config.json : |- { \"db\": { \"port\": 5432, \"username\": \"root\", \"database\": \"fossa\", \"host\": \"<some aws rds host>\", \"pool\": { \"maxConnections\": 5, \"minConnections\": 1, \"maxIdleTime\": 60000 } }, \"cache\": { \"package\": { \"engine\": \"s3\", \"store_private\": true, \"bucket\": \"<some-bucket>\", \"s3Options\": { \"endpoint\": \"s3.amazonaws.com\" } } }, In these cases you need to have a declarative defintion of the signature for the persistence claims. There is a CLI tool to help research dc-repo configuration. Local configuration make build ./bin/claims --help Docker run docker run ghcr.io/seizadi/claims --help You can run it like this: claims search --dir <path to k8 manifests> --stage dev --env env-4 --app some-app s3 rds amazonaws.com Here is a sample, showing how you can use the tool to determine that a claim for a resource is shared by two different environments: \u276f ./bin/claims search --dir /Users/seizadi/projects/src/github.com/seizadi/k8s.manifests some-resource.amazonaws.com some-resource.amazonaws.com [ dev/env-4/some-app integration/env-2/another-app ] Try to visualize this data using graph DB: docker run -p7474:7474 -p7687:7687 -e NEO4J_AUTH = neo4j/s3cr3t neo4j A bit more advanced: docker run \\ --name testneo4j \\ -p7474:7474 -p7687:7687 \\ -d \\ -v $HOME /neo4j/data:/data \\ -v $HOME /neo4j/logs:/logs \\ -v $HOME /neo4j/import:/var/lib/neo4j/import \\ -v $HOME /neo4j/plugins:/plugins \\ --env NEO4J_AUTH = neo4j/s3cr3t \\ neo4j:latest In case it is not clear why we are thinking about this as a graph problem. In particular for Neo4J, we want to create a record, where each record will contain information on :App and :Resource nodes, and the :ACCESS relationship. The :ACCESS relationship will have boolean .Props Read and Write to indicate access types. Both App and Resources would have name .Props. For Resources we would define .Props Type with values like (ObjectStore or Database). You can use this tool to populate the database and then using the graph query editor to visulaize the data match(n:App)-[r:ACCESS]->(m:Resource) return n,r,m Then to filter to only the dev stage: match(n:App)-[r:ACCESS]->(m:Resource) where n.stage = \"dev\" return n,r,m","title":"Home"},{"location":"#application-claim-docs","text":"","title":"Application Claim Docs"},{"location":"#introduction","text":"The motiviation for this project was to research application claim pattern. This pattern allows applciations to have a ligth weight claim that is used create a much more detailed resource. The deployment environment that is targeted is Kubernetes and the resources that are targeted by these claims are infrastructure resources like AWS RDS, S3 and other cloud provider services. The claim pattern shields the applications from knowing anything about the cloud provider concerns that are typically managed by a differents team that deploys the applciations and manages cloud infrastructure. There are examples of this pattern for example crossplane composition aims to support this claim pattern. Another example of this pattern is the db-controller and its DatabaseClaim . The former crossplane claim pattern is very general purpose and more passive while the db-controller claim is very specialized for database application and db-controller takes a more active role in managing the resource and provides database proxy and secret key rotation services. In this project we will research some other patterns, the goal is to find a common pattern(s) for application deployment.","title":"Introduction"},{"location":"#operators-versus-declarative-configuration","text":"In the following analysis we will consider declarative configuration of infrastructure from claims. This type of deployment might not fit some (or many) use cases where resources are shared and there is a need for reclaim policies enforced by an Operator, or there is specialized processing of the infrastructure resources required e.g. health monitoring or backup. It is intersting that crossplane composition reclaim lifecycle was backed out recently . We will examine this decision once we have found all our use cases that will require the claim pattern. In this project we will define CRDs for the claims that we will use in the declarative configuration. This will allow us to load them on the cluster and if necessary process them using an Operator. We will seed the project with the kubebuilder db-controller project scaffolding so we can use kubebuilder to build CRDs quickly but we will not implement any runtime logic in this proejct. The db-controller was setup for a single resource and we enable the project for multigroup layout: kubebuilder edit --multigroup true There is work to refactor the definition from /api to /apis directory but since this is PoC, I will not make these changes here. We want the claim data model to follow the following schema: %%{init: {'theme': 'base', 'themeVariables': { 'primaryColor': '#fffcbb', 'lineColor': '#ff0000', 'primaryBorderColor': '#ff0000'}}}%% graph LR A[[claims.atlas.infoblox.com]] --> B((ObjectStoreClaim)) A --> C((DataBaseClaim)) We could have finer grained grouping: %%{init: {'theme': 'base', 'themeVariables': { 'primaryColor': '#fffcbb', 'lineColor': '#ff0000', 'primaryBorderColor': '#ff0000'}}}%% graph LR A[[claims.atlas.infoblox.com]] --> B[[persistence.claims.atlas.infoblox.com]] B --> C((ObjectStoreClaim)) B --> D((DataBaseClaim)) For now we go with the coarse model, as most of the work is defining the leaf nodes. We can refactor to the finer grain model once we have a better idea of the different claims needed for the applications. We start by creating a claim for ObjectStore which we would map to AWS S3 resource for example: kubebuilder create api --controller false --kind ObjectStoreClaim --version v1 --group objectstore.claims.atlas.infoblox.com make update_crds","title":"Operators versus Declarative Configuration"},{"location":"#additional-patterns","text":"Generally infrastructure resources are injected into applications using configmap or secrets. In addition to the pattern highlighted above we will look at two other patterns that build on the established configmap or secret pattern. One problem with these patterns is that the namespace and associated configmap or secret must be present for the application deployment to succeed as the dependency is not present in the helm chart that is resolved. The two approaches we will make is to: - Use a workflow engine to process application defintion e.g. helm chart and use the application claim to create the create the resource - Custom program that understands applications and the environment consumes application helm charts and emits resolved mainfest that coverts claims to resources In general this the state flow in handling all the applications, in both approaches: %%{init: {'theme': 'base', 'themeVariables': { 'primaryColor': '#fffcbb', 'lineColor': '#ff0000', 'primaryBorderColor': '#ff0000'}}}%% stateDiagram-v2 [*] --> ProcessDC ProcessDC --> ProcessNextApp ProcessNextApp --> CheckNextApp state CheckNextApp < > CheckNextApp --> ProcessApp: if NextApp != \"\" CheckNextApp --> [*] : if NextApp == \"\" ProcessApp --> FetchChart FetchChart --> ExpandChart ExpandChart --> Customize Customize --> GitCheckIn GitCheckIn --> ProcessNextApp","title":"Additional Patterns"},{"location":"#workflow-approach","text":"In the workflow approach we would pick a solution that has a decralative workflow engine, two open source solutions are good examples: - Kubevela - Argo Workflow The two workflows solutions have different trades offs, kubevela is more specialized to CI/CD delivery has native integration with helm and other tooling using its plugin architecture, while argo workflow is more generalized and flexible in building workflows. The kubevela schema defintion could be a benefit if its fits your deployment model or problem as you have to define and model your configuration.","title":"Workflow Approach"},{"location":"#custom-approach","text":"In the custom approach we will build an application that understands the data model for our deployment configuration repo (dc-repo), see this example of dc-repo . This custom solution can still use open source tooling like kpt or QUE for declartive defintion and processing of the helm chart and creating resource configuration. We could leverage QUE integration in kubevela or develope our own custom integration . The kpt book and poc are good resources. We will focus on kpt helm support as this will be the source of our application defintion. I will focus on kpt and how it can be integrated, as I feel that the QUE language and integration in kubevela is better understood at this time. The kpt tool needs to have a resolved manifest for it to operate. We can step through the test CMDB repo, note with CMDB when built for local testing that postgres local server is run with the deployment, when run in the cloud it is run with a claim. To enable this we set postgresql.enabled to false: mkdir deploy cd tmp git clone git@github.com:seizadi/cmdb.git mkdir build cd build helm template --set postgresql.enabled = false -n cmdb ../cmdb/repo/cmdb/. --output-dir . At this point this is what our directory structure would look like: \u276f tree cmdb cmdb \u2514\u2500\u2500 templates \u251c\u2500\u2500 configmap.yaml \u251c\u2500\u2500 deployment.yaml \u251c\u2500\u2500 ingress.yaml \u251c\u2500\u2500 migrations.yaml \u251c\u2500\u2500 ns.yaml \u251c\u2500\u2500 postgres-claim.yaml \u251c\u2500\u2500 service.yaml \u2514\u2500\u2500 serviceaccount.yaml We are really intersted in the claim in postgres-claim.yaml: --- # Source: cmdb/templates/postgres-claim.yaml apiVersion : database.example.org/v1alpha1 kind : PostgresInstance metadata : name : RELEASE-NAME-cmdb namespace : cmdb spec : parameters : storageGB : 20 writeConnectionSecretToRef : name : RELEASE-NAME-cmdb-postgres-con The goal now is to kpt to process this claim and convert it to a infrastructure claim. We will use the kpt function pattern to search and replace the claim above with an infrastructure defintion that looks like this: apiVersion : database.aws.crossplane.io/v1beta1 kind : RDSInstance metadata : name : rdspostgresql spec : forProvider : region : us-west-1 vpcSecurityGroupIDRefs : - name : seizadi-bloxinabox-rds-sg dbSubnetGroupNameRef : name : seizadi-bloxinabox-rds-subnetgroup dbInstanceClass : <shape goes here> masterUsername : masteruser allocatedStorage : <minStorageGB goes here> engine : postgres engineVersion : \"12.8\" skipFinalSnapshotBeforeDeletion : true publiclyAccessible : false # enableIAMDatabaseAuthentication: true writeConnectionSecretToRef : namespace : <claim namespace goes here> name : <writeConnectionSecretToRef goes here> providerConfigRef : name : default The kpt function catalog is a good place to look for features to build for our required mutations. We can do some processing of the helm chart as needed like mutating the namespace for the application or setting it release name: kpt fn eval --image gcr.io/kpt-fn/search-replace:v0.2.0 -- by-path = 'metadata.namespace' put-value = 'cmdb' kpt fn eval --image gcr.io/kpt-fn/search-replace:v0.2.0 -- by-path = 'metadata.name' put-value = 'cmdb-dev-seizadi' You should see changes like this: diff --git a/deploy/build/cmdb/templates/configmap.yaml b/deploy/build/cmdb/templates/configmap.yaml index 3ad3e33..813bf80 100644 --- a/deploy/build/cmdb/templates/configmap.yaml +++ b/deploy/build/cmdb/templates/configmap.yaml @@ -3,7 +3,7 @@ apiVersion: v1 kind: ConfigMap metadata: name: RELEASE-NAME-cmdb - namespace: default + namespace: cmdb We try to do the claim mutation by using apply-replacements function which uses the kustomize synthax for replacements. apiVersion : kpt.dev/v1 kind : Kptfile metadata : name : claim pipeline : mutators : - image : gcr.io/kpt-fn/apply-replacements:unstable configPath : replacements.yaml We will define a kpt file to detail the desired changes: apiVersion : kpt.dev/v1 kind : Kptfile metadata : name : claim pipeline : mutators : - image : gcr.io/kpt-fn/apply-replacements:unstable configPath : replacements.yaml Now we can process the claim: kpt fn render claim Now we can check what changed in the infrastructure defintion was updated: diff --git a/deploy/claim/infrastructure.yaml b/deploy/claim/infrastructure.yaml index e705744..ad2dce9 100644 --- a/deploy/claim/infrastructure.yaml +++ b/deploy/claim/infrastructure.yaml @@ -10,16 +10,16 @@ spec: - name: seizadi-bloxinabox-rds-sg dbSubnetGroupNameRef: name: seizadi-bloxinabox-rds-subnetgroup - dbInstanceClass: <shape goes here> + dbInstanceClass: db.t2.small masterUsername: masteruser - allocatedStorage: <minStorageGB goes here> + allocatedStorage: 20 engine: postgres engineVersion: \"12.8\" skipFinalSnapshotBeforeDeletion: true publiclyAccessible: false # enableIAMDatabaseAuthentication: true writeConnectionSecretToRef: - namespace: <claim namespace goes here> - name: <writeConnectionSecretToRef goes here> + namespace: cmdb + name: RELEASE-NAME-cmdb-postgres-con providerConfigRef: name: default This shows simple mutations but there might be cases where you need to implement conditional logic or more complex mutations rather than replace/substitutions. For example see the more complex operations available to crossplane compositions to transform claims. In these cases in the case of kpt we have starlark scripting . starlark is is a dialect of Python. It is commonly used as a configuration language. It is an untyped dynamic language with high-level data types, first-class functions with lexical scope, and garbage collection. You can find the spec for the Starlark language here . You can also find its API reference here . We will do a example of claim transformation using this method as well. POC USING STARLARK SCRIPT GOES HERE!","title":"Custom Approach"},{"location":"#application-use-cases","text":"Lets look at some common use cases below","title":"Application Use Cases"},{"location":"#container-args","text":"In this use case you have something like this: containers : - name : <some-container> ..... args : - --s3-region=<some region> - --s3-bucket=<some bucket> - --s3-folder=<some folder> ..... There is also the associated secret and you can have this injected via Secrets, use node IAM permission, kube2iam or IRSA. This declaration does not tell us if the S3 resource is shared by other applciations. If it is shared does not application need read only or read/write access to the resource. The application claim should also include these intents. The clients will use AWS SDK to access the object store, we have to test other cloud providers but they are suppose to be able to support the API calls with their own endpoints: import ( \"github.com/aws/aws-sdk-go/aws\" \"github.com/aws/aws-sdk-go/aws/session\" \"github.com/aws/aws-sdk-go/service/s3\" \"github.com/aws/aws-sdk-go/service/s3/s3manager\" ) ... awsConfig := & aws . Config { Region : aws . String ( s3 - region ), HTTPClient : & http . Client { Timeout : defaultHTTPTimeout }, Endpoint : & config . Endpoint , } awsSession , _ := session . NewSession ( awsConfig ) svc = s3 . New ( awsSession ) downloader = s3manager . NewDownloader ( awsSession ) ... The ConfigMap pattern is another one found in applications, you can refrence the open source solution Teleport in how it pulls persistence information for its configuration: --- # Source: teleport/templates/config.yaml apiVersion : v1 kind : ConfigMap metadata : name : teleport data : teleport.yaml : | teleport: log: output: stderr severity: DEBUG data_dir: /var/lib/teleport auth_servers: - teleport.ib-system.svc.cluster.local:3025 - auth-poseidon-preprod.csp.infoblox.com:3025 storage: audit_events_uri: - \"dynamodb://<some-dynamodb>\" audit_sessions_uri: \"s3://<some-bucket>\" Another example similarly for configuration of FOSSA using ConfigMap: --- # Source: fossa-core/templates/config/config-configmap.yaml apiVersion : v1 kind : ConfigMap metadata : name : fossa-config namespace : fossa data : config.json : |- { \"db\": { \"port\": 5432, \"username\": \"root\", \"database\": \"fossa\", \"host\": \"<some aws rds host>\", \"pool\": { \"maxConnections\": 5, \"minConnections\": 1, \"maxIdleTime\": 60000 } }, \"cache\": { \"package\": { \"engine\": \"s3\", \"store_private\": true, \"bucket\": \"<some-bucket>\", \"s3Options\": { \"endpoint\": \"s3.amazonaws.com\" } } }, In these cases you need to have a declarative defintion of the signature for the persistence claims. There is a CLI tool to help research dc-repo configuration. Local configuration make build ./bin/claims --help Docker run docker run ghcr.io/seizadi/claims --help You can run it like this: claims search --dir <path to k8 manifests> --stage dev --env env-4 --app some-app s3 rds amazonaws.com Here is a sample, showing how you can use the tool to determine that a claim for a resource is shared by two different environments: \u276f ./bin/claims search --dir /Users/seizadi/projects/src/github.com/seizadi/k8s.manifests some-resource.amazonaws.com some-resource.amazonaws.com [ dev/env-4/some-app integration/env-2/another-app ] Try to visualize this data using graph DB: docker run -p7474:7474 -p7687:7687 -e NEO4J_AUTH = neo4j/s3cr3t neo4j A bit more advanced: docker run \\ --name testneo4j \\ -p7474:7474 -p7687:7687 \\ -d \\ -v $HOME /neo4j/data:/data \\ -v $HOME /neo4j/logs:/logs \\ -v $HOME /neo4j/import:/var/lib/neo4j/import \\ -v $HOME /neo4j/plugins:/plugins \\ --env NEO4J_AUTH = neo4j/s3cr3t \\ neo4j:latest In case it is not clear why we are thinking about this as a graph problem. In particular for Neo4J, we want to create a record, where each record will contain information on :App and :Resource nodes, and the :ACCESS relationship. The :ACCESS relationship will have boolean .Props Read and Write to indicate access types. Both App and Resources would have name .Props. For Resources we would define .Props Type with values like (ObjectStore or Database). You can use this tool to populate the database and then using the graph query editor to visulaize the data match(n:App)-[r:ACCESS]->(m:Resource) return n,r,m Then to filter to only the dev stage: match(n:App)-[r:ACCESS]->(m:Resource) where n.stage = \"dev\" return n,r,m","title":"Container Args"}]}